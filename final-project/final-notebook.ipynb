{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Deep Q Network\n",
    "\n",
    "paper accepted at NIPS 2017 Bayesian Deep Learning workshop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Games | Robotics | Finance |\n",
    "- | - | - | \n",
    "<img src=\"img/alphago.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"img/robotics.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"img/finance.png\" alt=\"Drawing\" style=\"width: 200px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP) and Reinforcement Learning (RL)\n",
    "\n",
    "In Markov Decision Process (MDP), an agent is in state $s_t \\in \\mathcal{S}$, takes action $a_t \\in \\mathcal{A}$, then transitions to state $s_{t+1} \\in \\mathcal{S}$ and receives instant reward $r_t$. A mapping from state to action $\\pi: \\mathcal{S} \\mapsto \\mathcal{A}$ is a policy. The aim is to find policy to optimize cumulative reward\n",
    "$$\\mathbb{E}_\\pi\\big[ \\sum_{t=0}^\\infty r_t\\gamma^t  \\big]$$\n",
    "where $\\gamma$ is a discount factor. \n",
    "\n",
    "#### Action value function $Q^\\pi(s,a)$\n",
    "An agent starts with state action pair $s_0 = s,a_0 = a$ then follows policy $\\pi$, then the action value function is defined as $$Q^\\pi(s,a) = \\mathbb{E}_\\pi \\big[ \\sum_{t=0}^\\infty r_t \\gamma^t \\big| s_0 = s, a_0 = a \\big]$$\n",
    "\n",
    "#### Bellman quation of optimality\n",
    "The optimal policy $\\pi^\\ast$ has optimal action value function $Q^\\ast(s,a)$ satisfy the following equality $$Q^\\ast(s_t,a_t) = \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q^\\ast(s_{t+1},a) \\big]$$\n",
    "To obtain $\\pi^\\ast$ from $Q^\\ast(s,a)$: $\\pi^\\ast(s) = \\arg\\max_a Q^\\ast(s,a)$ (one-step lookahead).\n",
    "\n",
    "#### Bellman error\n",
    "One way to evaluate how suboptimal a policy is, is through Bellman error. Bellman error for state action pair $(s,a)$ under policy $\\pi$ is $$(Q^\\pi(s_t,a_t) - \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q^\\pi(s_{t+1},a) \\big])^2$$\n",
    "\n",
    "Notice that optimal $Q^\\ast(s,a)$ has exact zero Bellman error for all $(s,a)$. A policy $\\pi$ with zero Bellman error is also optimal. Suboptimal policies tend to have small Bellman error (though not necessarily).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network (DQN)\n",
    "Deep Q Network is proposed in [Mnih, 2013], as one of the first successful deep RL framework to solve challenging control tasks. Consider parameterizing a neural network $Q_\\theta(s,a)$ with parameter $\\theta$ to approximate $Q^\\ast(s,a)$. Use SGD to minimize the objective\n",
    "$$\\mathbb{E}_{\\pi_\\theta} \\big[(Q_\\theta(s_t,a_t) - \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q_\\theta(s_{t+1},a) \\big])^2\\big]$$\n",
    "\n",
    "If $Q_\\theta(s,a)$ minimizes the above objective then potentially $Q_\\theta(s,a) \\approx Q^\\ast(s,a)$ and greedy policy w.r.t. $Q_\\theta(s,a)$ is close to optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration in RL\n",
    "Exploration is essential in solving RL tasks. Being greedy usually gives suboptimal performance because the agent does not explore and cannot find potentially more optimal patterns. Efficient exploration scheme is critical in solving challenging control tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Deep Q Network (VDQN)\n",
    "Original DQN uses $\\epsilon-$greedy exploration. Such exploration scheme is myopic and is not efficient enough in challenging tasks. Variational DQN maintains a distribution over DQN parameters $\\theta \\sim q_\\phi(\\theta)$ with parameter $\\phi$.\n",
    "\n",
    "Proposed objective: $$\\mathbb{E}_{\\theta\\sim q_\\phi(\\theta)} \\big[ \\mathbb{E}_{\\pi_\\theta} \\big[(Q_\\theta(s_t,a_t) - \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q_\\theta(s_{t+1},a) \\big])^2\\big]  \\big] - \\lambda \\mathbb{H}(q_\\phi(\\theta))$$\n",
    "\n",
    "First term encourages minimization of expected Bellman error -- search for optimal policy; Second term encourages high entropy of distribution $q_\\phi(\\theta)$ -- encourage exploration. \n",
    "\n",
    "In practice, the 'regression target' $d_t = \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q_\\theta(s_{t+1},a)\\big]$ is computed by a target network. Let targets $D = \\{d_t\\}$ be data. We can interpret $Q_\\theta(s,a)$ as a Bayesian neural network with uniform improper prior $p(\\theta) \\propto 1$, the above objective reduces to \n",
    "$$\\mathbb{KL}\\big[q_\\phi(\\theta) \\ || \\ p(\\theta|D)\\big]$$\n",
    "we can use Edward to update distribution parameter $\\phi$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below are all necessary packages\n",
    "import edward                # probabilistic programming package\n",
    "import tensorflow as tf      # autodiff package\n",
    "import chainer               # autodiff package\n",
    "import gym                   # OpenAI gym for RL environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments: VDQN architecture\n",
    "\n",
    "For control tasks in current experiments, we use feed forward neural network. The input is state $s_t$ at each time step. The feed forward network has 2 hidden layers each with $100$ hidden units with relu activation. The output is a probability vector $p$ of choosing actions (discrete action space).\n",
    "\n",
    "In all experiments, VDQN applies KLqp inference with $\\lambda = 0.02$. MAP inference recovers conventional DQN. Any inference algorithm is approximate minimization of Bellman error. VDQN parameterizes the distribubtion as component-wise gaussian with mean and variance parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Environment\n",
    "### OpenAI gym control tasks\n",
    "Learn policies to manipulate mechanical systems. \n",
    "\n",
    "MountainCar | CartPole | Acrobot |\n",
    "- | - | - | \n",
    "<img src=\"img/mountaincarimg.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"img/cartpoleimg.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"img/acrobotimg.jpg\" alt=\"Drawing\" style=\"width: 200px;\"/> \n",
    "\n",
    "### Chain MDP \n",
    "Learn policies to reach far-away destination without getting trapped in local optimality. Benchmark for exploration. Take exponential time to even reach the optimal state for random exploration strategy.\n",
    "\n",
    "<img src=\"img/chainMDP.png\" alt=\"Drawing\" style=\"width: 500px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI gym control tasks\n",
    "\n",
    "Compare DQN with VDQN. Both agents have the same architecture. DQN applies $\\epsilon-$greedy exploration with $\\epsilon = 0.1$. VDQN applies KLqp inference with $\\lambda = 0.02$.\n",
    "\n",
    "#### Observations\n",
    "1. Both algorithms solve control tasks within given number of iterations\n",
    "2. VDQN makes progress faster but tends to converge more slowly on a certain tasks. Potentially because the exploration prevents fast convergence to optimal policy.\n",
    "\n",
    "CartPole-v0 | CartPole-v1 | MountainCar | Acrobot |\n",
    "- | - | - | - | \n",
    "<img src=\"img/CartPole-v0.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"img/CartPole-v1.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"img/MountainCar-v0.png\" alt=\"Drawing\" style=\"width: 200px;\"/> | <img src=\"img/Acrobot-v1.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain MDP\n",
    "\n",
    "Compare DQN, VDQN and NoisyNet. All agents have the same architecture. DQN applies $\\epsilon-$greedy exploration with $\\epsilon = 0.1$. VDQN applies KLqp inference with $\\lambda = 0.02$. VDQN and NoisyNet starts out with the same gaussian distribution parameters for each DQN parameter.\n",
    "\n",
    "#### Observations\n",
    "1. All algorihtms solve the small $N$ Chain MDP within given numbber of iterations, NoisyNet and VDQN have similar training curves.\n",
    "2. For medium size $N$, DQN does not make progress. NoisyNet displays high variance in performance. VDQN solves the task successfully.\n",
    "3. DQN and NoisyNet both get stuck. VDQN solves the task with higher variance.\n",
    "\n",
    "\n",
    "$N=5$ | $N=50$ | $N=100$ | \n",
    "- | - | - | \n",
    "<img src=\"img/MDPN5_NoisyNet.png\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/MDPN50_NoisyNet.png\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/MDPN100_NoisyNet.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain MDP: Measuring Exploration -- State Visitation Counts\n",
    "\n",
    "Plot state visitation counts below to illustrate the exploration efficiency of three algorithmd. For a given state $s_i$, $1\\leq i\\leq N$, if it is ever visited in one episode we set $c_i = 1$. The plot is a moving average of $c_i$ over time, to approximate the probability of visiting state $i$ under current policy. You need to visit the states frequently enough to learn a good policy!\n",
    "\n",
    "Illustrate three curves\n",
    "1. $s_1$: locally optimal, see if the agent is stuck\n",
    "2. $s_\\frac{N}{2}$: see if the agent ever crosses $s_{N/2}$ to explore another half of the chain\n",
    "3. $s_N$: global optimal, see if the agent explores consistently\n",
    "\n",
    "#### Observations\n",
    "1. DQN visits $s_1$ increasingly frequently and gets stuck at local optimum. Occasionally, DQN goes over $s_{\\frac{N}{2}}$ (green spike) and does not have enough momentum ( consisntecy in exploration) to reach $s_N$.\n",
    "2. NoisyNet has more consistency in exploration as illustrated in $N = 32$ case. For $N = 128$, however, the exploration is much slower (blue and green spike at the end show that NoisyNet could potentially solve the problem, while taking much more time)\n",
    "3. VDQN explores the most consistently. VDQN maintains a non-trivial visitation probability from the start of training. It can then visit $s_N$ for sufficient number of times and quickly converge to optimal policy.\n",
    "\n",
    "\n",
    "DQN $N = 32$ | NoisyNet $N = 32$ | VDQN $N = 32$ | \n",
    "- | - | - | \n",
    "<img src=\"img/DQN_MDPN32visit.png\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/NoisyNet_MDPN32visit.png\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/VIDQN_MDPN32visit.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "DQN $N = 128$ | NoisyNet $N = 128$ | VDQN $N = 128$ | \n",
    "- | - | - | \n",
    "<img src=\"img/DQN_MDPN128visit.png\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/NoisyNet_MDPN100visit.png\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/VIDQN_MDPN128visit.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why a potentially better exploration scheme?\n",
    "\n",
    "### Comparison across algorithms\n",
    "1. DQN explores by randomization of single actions. Such exploration breaks temporal correlation in the current policy. Feedback not consistent.\n",
    "2. NoisyNet explores by randomization in policy space. Feedback is consistent, exploration is more consistent. May get stuck due to premature convergence.\n",
    "3. VDQN explicitly encourages high entropy in distribubtion over policies. This encourages the agent to achieve low expected Bellman error, while encompassing as many policies as possible -- consistent and persistent exploration. May slow down learning.\n",
    "\n",
    "### Connection to Thompson sampling\n",
    "Thompson sampling is a successful exploration scheme in multi-arm bandit and RL. The idea is to maintain a widely spread belief (prior) over MDPs. Take action and then updata belief (posterior). This strikes a nice balance between exploration vs. exploitation.\n",
    "\n",
    "VDQN executes approximate Thompson sampling, approximation comes from\n",
    "1. Posterior sampling is not exact, replaced by variational approximation\n",
    "2. Target values are not from optimal value function, but from target network\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
