{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Deep Q Network\n",
    "\n",
    "paper accepted at NIPS 2017 Bayesian Deep Learning workshop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP) and Reinforcement Learning (RL)\n",
    "\n",
    "In Markov Decision Process (MDP), an agent is in state $s_t \\in \\mathcal{S}$, takes action $a_t \\in \\mathcal{A}$, then transitions to state $s_{t+1} \\in \\mathcal{S}$ and receives instant reward $r_t$. A mapping from state to action $\\pi: \\mathcal{S} \\mapsto \\mathcal{A}$ is a policy. The aim is to find policy to optimize cumulative reward\n",
    "$$\\mathbb{E}_\\pi\\big[ \\sum_{t=0}^\\infty r_t\\gamma^t  \\big]$$\n",
    "where $\\gamma$ is a discount factor. \n",
    "\n",
    "#### Action value function $Q^\\pi(s,a)$\n",
    "An agent starts with state action pair $s_0 = s,a_0 = a$ then follows policy $\\pi$, then the action value function is defined as $$Q^\\pi(s,a) = \\mathbb{E}_\\pi \\big[ \\sum_{t=0}^\\infty r_t \\gamma^t \\big| s_0 = s, a_0 = a \\big]$$\n",
    "\n",
    "#### Bellman quation of optimality\n",
    "The optimal policy $\\pi^\\ast$ has optimal action value function $Q^\\ast(s,a)$ satisfy the following equality $$Q^\\ast(s_t,a_t) = \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q^\\ast(s_{t+1},a) \\big]$$\n",
    "To obtain $\\pi^\\ast$ from $Q^\\ast(s,a)$: $\\pi^\\ast(s) = \\arg\\max_a Q^\\ast(s,a)$ (one-step lookahead).\n",
    "\n",
    "#### Bellman error\n",
    "One way to evaluate how suboptimal a policy is, is through Bellman error. Bellman error for state action pair $(s,a)$ under policy $\\pi$ is $$(Q^\\pi(s_t,a_t) - \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q^\\pi(s_{t+1},a) \\big])^2$$\n",
    "\n",
    "Notice that optimal $Q^\\ast(s,a)$ has exact zero Bellman error for all $(s,a)$. A policy $\\pi$ with zero Bellman error is also optimal. Suboptimal policies tend to have small Bellman error (though not necessarily).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q Network (DQN)\n",
    "Deep Q Network is proposed in [Mnih, 2013], as one of the first successful deep RL framework to solve challenging control tasks. Consider parameterizing a neural network $Q_\\theta(s,a)$ with parameter $\\theta$ to approximate $Q^\\ast(s,a)$. Use SGD to minimize the objective\n",
    "$$\\mathbb{E}_{\\pi_\\theta} \\big[(Q_\\theta(s_t,a_t) - \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q_\\theta(s_{t+1},a) \\big])^2\\big]$$\n",
    "\n",
    "If $Q_\\theta(s,a)$ minimizes the above objective then potentially $Q_\\theta(s,a) \\approx Q^\\ast(s,a)$ and greedy policy w.r.t. $Q_\\theta(s,a)$ is close to optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration in RL\n",
    "Exploration is essential in solving RL tasks. Being greedy usually gives suboptimal performance because the agent does not explore and cannot find potentially more optimal patterns. Efficient exploration scheme is critical in solving challenging control tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Deep Q Network (VDQN)\n",
    "Original DQN uses $\\epsilon-$greedy exploration. Such exploration scheme is myopic and is not efficient enough in challenging tasks. Variational DQN maintains a distribution over DQN parameters $\\theta \\sim q_\\phi(\\theta)$ with parameter $\\phi$.\n",
    "\n",
    "Proposed objective: $$\\mathbb{E}_{\\theta\\sim q_\\phi(\\theta)} \\big[ \\mathbb{E}_{\\pi_\\theta} \\big[(Q_\\theta(s_t,a_t) - \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q_\\theta(s_{t+1},a) \\big])^2\\big]  \\big] - \\lambda \\mathbb{H}(q_\\phi(\\theta))$$\n",
    "\n",
    "First term encourages minimization of expected Bellman error -- search for optimal policy; Second term encourages high entropy of distribution $q_\\phi(\\theta)$ -- encourage exploration. \n",
    "\n",
    "In practice, the 'regression target' $d_t = \\max_a \\mathbb{E}\\big[ r_t + \\gamma Q_\\theta(s_{t+1},a)\\big]$ is computed by a target network. Let targets $D = \\{d_t\\}$ be data. We can interpret $Q_\\theta(s,a)$ as a Bayesian neural network with uniform improper prior $p(\\theta) \\propto 1$, the above objective reduces to \n",
    "$$\\mathbb{KL}\\big[q_\\phi(\\theta) \\ || \\ p(\\theta|D)\\big]$$\n",
    "we can use Edward to update distribution parameter $\\phi$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edward\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VDQN architecture\n",
    "\n",
    "For control tasks in current experiments, we use feed forward neural network. The input is state $s_t$ at each time step. The feed forward network has 2 hidden layers each with $100$ hidden units with relu activation. The output is a probability vector $p$ of choosing actions (discrete action space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 100\n",
      "Number of features: 5\n"
     ]
    }
   ],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "data = np.loadtxt('data/crabs_train.txt', delimiter=',')\n",
    "data[data[:, 0] == -1, 0] = 0  # replace -1 label with 0 label\n",
    "\n",
    "N = data.shape[0]  # number of data points\n",
    "D = data.shape[1] - 1  # number of features\n",
    "\n",
    "X_train = data[:, 1:]\n",
    "y_train = data[:, 0]\n",
    "\n",
    "print(\"Number of data points: {}\".format(N))\n",
    "print(\"Number of features: {}\".format(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "A Gaussian process is a powerful object for modeling nonlinear\n",
    "relationships between pairs of random variables. It defines a distribution over\n",
    "(possibly nonlinear) functions, which can be applied for representing\n",
    "our uncertainty around the true functional relationship.\n",
    "Here we define a Gaussian process model for classification\n",
    "(Rasumussen & Williams, 2006).\n",
    "\n",
    "Formally, a distribution over functions $f:\\mathbb{R}^D\\to\\mathbb{R}$ can be specified\n",
    "by a Gaussian process\n",
    "$$\n",
    "\\begin{align*}\n",
    "  p(f)\n",
    "  &=\n",
    "  \\mathcal{GP}(f\\mid \\mathbf{0}, k(\\mathbf{x}, \\mathbf{x}^\\prime)),\n",
    "\\end{align*}\n",
    "$$\n",
    "whose mean function is the zero function, and whose covariance\n",
    "function is some kernel which describes dependence between\n",
    "any set of inputs to the function.\n",
    "\n",
    "Given a set of input-output pairs\n",
    "$\\{\\mathbf{x}_n\\in\\mathbb{R}^D,y_n\\in\\mathbb{R}\\}$,\n",
    "the likelihood can be written as a multivariate normal\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{y})\n",
    "  &=\n",
    "  \\text{Normal}(\\mathbf{y} \\mid \\mathbf{0}, \\mathbf{K})\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{K}$ is a covariance matrix given by evaluating\n",
    "$k(\\mathbf{x}_n, \\mathbf{x}_m)$ for each pair of inputs in the data\n",
    "set.\n",
    "\n",
    "The above applies directly for regression where $\\mathbb{y}$ is a\n",
    "real-valued response, but not for (binary) classification, where $\\mathbb{y}$\n",
    "is a label in $\\{0,1\\}$. To deal with classification, we interpret the\n",
    "response as latent variables which is squashed into $[0,1]$. We then\n",
    "draw from a Bernoulli to determine the label, with probability given\n",
    "by the squashed value.\n",
    "\n",
    "Define the likelihood of an observation $(\\mathbf{x}_n, y_n)$ as\n",
    "\n",
    "\\begin{align*}\n",
    "  p(y_n \\mid \\mathbf{z}, x_n)\n",
    "  &=\n",
    "  \\text{Bernoulli}(y_n \\mid \\text{logit}^{-1}(\\mathbf{x}_n^\\top \\mathbf{z})).\n",
    "\\end{align*}\n",
    "\n",
    "Define the prior to be a multivariate normal\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{z})\n",
    "  &=\n",
    "  \\text{Normal}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{K}),\n",
    "\\end{align*}\n",
    "\n",
    "with covariance matrix given as previously stated.\n",
    "\n",
    "Let's build the model in Edward. We use a radial basis function (RBF)\n",
    "kernel, also known as the squared exponential or exponentiated\n",
    "quadratic. It returns the kernel matrix evaluated over all pairs of\n",
    "data points; we then Cholesky decompose the matrix to parameterize the\n",
    "multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "f = MultivariateNormalTriL(loc=tf.zeros(N), scale_tril=tf.cholesky(rbf(X)))\n",
    "y = Bernoulli(logits=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a placeholder `X`. During inference, we pass in\n",
    "the value for this placeholder according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Perform variational inference.\n",
    "Define the variational model to be a fully factorized normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qf = Normal(loc=tf.Variable(tf.random_normal([N])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([N]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run variational inference for `500` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 9s | Loss: 82.755\n"
     ]
    }
   ],
   "source": [
    "inference = ed.KLqp({f: qf}, data={X: X_train, y: y_train})\n",
    "inference.run(n_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case\n",
    "`KLqp` defaults to minimizing the\n",
    "$\\text{KL}(q\\|p)$ divergence measure using the reparameterization\n",
    "gradient.\n",
    "For more details on inference, see the [$\\text{KL}(q\\|p)$ tutorial](/tutorials/klqp).\n",
    "(This example happens to be slow because evaluating and inverting full\n",
    "covariances in Gaussian processes happens to be slow.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
